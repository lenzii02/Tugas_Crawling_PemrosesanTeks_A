{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU9YfdenkuZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# === Unduh NLTK data ===\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "print(\"‚úì Download selesai!\\n\")\n",
        "\n",
        "# === Inisialisasi Stopwords ===\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# === Fungsi Preprocessing tanpa Stemming ===\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return [], \"\", 0\n",
        "\n",
        "    # 1Ô∏è‚É£ Bersihkan HTML tag dan noise\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 2Ô∏è‚É£ Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3Ô∏è‚É£ Tokenizing\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 4Ô∏è‚É£ Stopword removal\n",
        "    tokens = [word for word in tokens if word not in stop_words and word != \"\"]\n",
        "\n",
        "    # 5Ô∏è‚É£ Gabungkan kembali jadi string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    word_count = len(tokens)\n",
        "\n",
        "    return tokens, cleaned_text, word_count\n",
        "\n",
        "# === Baca file Excel ===\n",
        "input_file = '/content/quora_opinion_israel.xlsx'\n",
        "print(f\"üìÇ Membaca file: {input_file}\")\n",
        "df = pd.read_excel(input_file)\n",
        "print(f\"‚úì Berhasil membaca {len(df)} baris data\\n\")\n",
        "\n",
        "# === Pilih kolom teks ===\n",
        "text_column = 'opinion' if 'opinion' in df.columns else df.columns[0]\n",
        "print(f\"Menggunakan kolom: '{text_column}' untuk preprocessing\\n\")\n",
        "\n",
        "# === Jalankan preprocessing ===\n",
        "print(\"üîÑ Memproses teks...\")\n",
        "processed = df[text_column].apply(preprocess_text)\n",
        "df['tokens'] = processed.apply(lambda x: x[0])\n",
        "df['clean_text'] = processed.apply(lambda x: x[1])\n",
        "df['word_count'] = processed.apply(lambda x: x[2])\n",
        "print(\"‚úì Preprocessing selesai!\\n\")\n",
        "\n",
        "# === Tampilkan contoh hasil ===\n",
        "print(\"=\"*80)\n",
        "print(\"CONTOH HASIL PREPROCESSING:\")\n",
        "print(\"=\"*80)\n",
        "for i in range(min(3, len(df))):\n",
        "    print(f\"\\nüìù Data ke-{i+1}:\")\n",
        "    print(f\"TOKENS      : {df['tokens'].iloc[i][:10]} ...\")\n",
        "    print(f\"CLEAN TEXT  : {df['clean_text'].iloc[i][:100]}...\")\n",
        "    print(f\"WORD COUNT  : {df['word_count'].iloc[i]}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# === Simpan ke Excel ===\n",
        "output_file = '/content/quora_opinion_israel_CLEANED.xlsx'\n",
        "print(f\"\\nüíæ Menyimpan hasil ke: {output_file}\")\n",
        "df_output = df[['tokens', 'clean_text', 'word_count']].copy()\n",
        "df_output.to_excel(output_file, index=False, engine='openpyxl')\n",
        "print(\"‚úì File berhasil disimpan!\")\n",
        "\n",
        "# === Verifikasi file ===\n",
        "print(\"\\nüîç Memverifikasi file yang tersimpan...\")\n",
        "df_verify = pd.read_excel(output_file)\n",
        "print(f\"‚úì File terverifikasi dengan {len(df_verify)} baris\")\n",
        "print(f\"‚úì Kolom: {list(df_verify.columns)}\")\n",
        "\n",
        "print(\"\\nüìÑ Sample dari file yang TERSIMPAN:\")\n",
        "print(df_verify.head(2).to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ PROSES SELESAI ‚Äî Tidak ada stemming, duplikasi kata dipertahankan per dokumen!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ]
}